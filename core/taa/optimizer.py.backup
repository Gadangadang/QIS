"""
TAA Portfolio Optimizers - Multiple Optimization Approaches.

Production Optimizer:
- TAAOptimizer: Full-featured with constraints, tracking error, turnover limits, etc.
  Used by BacktestOptimizer for production backtests.

Comparison Optimizers (simplified for testing different approaches):
- MeanVarianceOptimizer: Basic mean-variance (no constraints)
- MaxSharpeOptimizer: Maximize Sharpe ratio
- MinVarianceOptimizer: Minimize risk
- RiskParityOptimizer: Equal risk contribution
- BlackLittermanOptimizer: Blend ML predictions with market equilibrium
- CVaROptimizer: Tail risk minimization
- HRPOptimizer: Hierarchical Risk Parity
- KellyOptimizer: Growth optimal

Uses cvxpy for convex optimization with constraints.
"""

import numpy as np
import pandas as pd
import cvxpy as cp
from typing import Dict, Optional, Tuple
import logging
from scipy.optimize import minimize
from scipy.cluster.hierarchy import linkage

from .constraints import OptimizationConstraints

logger = logging.getLogger(__name__)


# =============================================================================
# PRODUCTION OPTIMIZER (Full-Featured)
# =============================================================================

class TAAOptimizer:
    """
    PRODUCTION mean-variance optimizer with full constraint support.
    
    This is the main optimizer used in production backtests. It includes:
    - Full OptimizationConstraints support (position, tracking error, turnover, transaction costs)
    - VIX-adjusted transaction costs
    - Multiple solver fallback (CLARABEL → OSQP → SCS)
    - Detailed metadata tracking
    
    Use this with BacktestOptimizer for production backtests.
    Use simplified optimizers (MeanVarianceOptimizer, etc.) for comparison testing.
    
    Objective:
        maximize: expected_return - risk_aversion * portfolio_variance - transaction_costs - turnover_penalty
    
    Subject to:
        - weights sum to 1 (fully invested or with cash)
        - position size limits (min/max per sector)
        - tracking error constraint (vs benchmark)
        - turnover constraint (vs previous weights)
    
    Example:
        >>> constraints = load_constraints_from_config('config/taa_constraints.yaml')
        >>> optimizer = TAAOptimizer(constraints)
        >>> weights, metadata = optimizer.optimize(
        ...     expected_returns={'XLK': 0.08, 'XLF': 0.05},
        ...     covariance_matrix=cov_matrix,
        ...     previous_weights={'XLK': 0.3, 'XLF': 0.2},
        ...     current_vix=18.5
        ... )
    """
    
    def __init__(self, constraints: OptimizationConstraints):
        """
        Initialize optimizer with constraints.
        
        Args:
            constraints: OptimizationConstraints object with all parameters
        """
        self.constraints = constraints
        self.constraints.validate_all()
    
    def optimize(
        self,
        expected_returns: Dict[str, float],
        covariance_matrix: pd.DataFrame,
        previous_weights: Optional[Dict[str, float]] = None,
        current_vix: Optional[float] = None
    ) -> Tuple[Dict[str, float], Dict[str, any]]:
        """
        Optimize portfolio weights.
        
        Args:
            expected_returns: Dict of ticker -> expected return (e.g., 0.08 for 8%)
            covariance_matrix: Covariance matrix of returns (tickers as index/columns)
            previous_weights: Previous portfolio weights (for turnover calculation)
            current_vix: Current VIX level (for transaction cost adjustment)
            
        Returns:
            Tuple of:
                - Dict[str, float]: Optimal weights (ticker -> weight)
                - Dict[str, any]: Optimization metadata (objective value, status, etc.)
        """
        # Convert inputs to arrays with consistent ordering
        tickers = list(expected_returns.keys())
        n_assets = len(tickers)
        
        mu = np.array([expected_returns[t] for t in tickers])
        Sigma = covariance_matrix.loc[tickers, tickers].values
        
        # Previous weights (default to equal weight if not provided)
        if previous_weights is None:
            w_prev = np.ones(n_assets) / n_assets
        else:
            w_prev = np.array([previous_weights.get(t, 0) for t in tickers])
        
        # Define optimization variable
        w = cp.Variable(n_assets)
        
        # Objective: maximize expected return - risk penalty - transaction costs - turnover penalty
        portfolio_return = mu @ w
        portfolio_variance = cp.quad_form(w, Sigma)
        
        # Turnover (sum of absolute changes)
        turnover = cp.sum(cp.abs(w - w_prev))
        
        # Transaction costs
        tc_model = self.constraints.transaction_costs
        tc_rate = (tc_model.commission_bps + tc_model.slippage_bps) / 10000
        if tc_model.vix_adjustment_enabled and current_vix and current_vix > tc_model.vix_threshold:
            tc_rate *= tc_model.vix_multiplier
        transaction_costs = tc_rate * turnover
        
        # Turnover penalty
        turnover_penalty = 0
        if self.constraints.turnover.penalize:
            turnover_penalty = self.constraints.turnover.penalty_lambda * turnover
        
        # Objective function
        objective = cp.Maximize(
            portfolio_return 
            - self.constraints.risk_aversion * portfolio_variance
            - transaction_costs
            - turnover_penalty
        )
        
        # Constraints list
        constraints_list = []
        
        # 1. Weights sum to 1
        constraints_list.append(cp.sum(w) == 1)
        
        # 2. Position limits
        pos_constr = self.constraints.position
        if pos_constr.long_only:
            constraints_list.append(w >= pos_constr.min_position)
        else:
            constraints_list.append(w >= -pos_constr.max_position)  # Allow shorts
        
        constraints_list.append(w <= pos_constr.max_position)
        
        # 3. Tracking error constraint
        if self.constraints.tracking_error.enabled:
            te_constr = self.constraints.tracking_error
            w_bench = np.array([te_constr.benchmark_weights.get(t, 0) for t in tickers])
            
            # Active weights
            w_active = w - w_bench
            
            # Tracking error: sqrt((w - w_bench)' * Sigma * (w - w_bench))
            # Constraint: TE <= max_te
            # Squared form: (w - w_bench)' * Sigma * (w - w_bench) <= max_te^2
            te_squared = cp.quad_form(w_active, Sigma)
            constraints_list.append(te_squared <= te_constr.max_te ** 2)
        
        # 4. Turnover constraint
        if self.constraints.turnover.enabled:
            max_turnover = self.constraints.turnover.max_monthly
            constraints_list.append(turnover <= max_turnover)
        
        # Solve optimization problem
        problem = cp.Problem(objective, constraints_list)
        
        # Try solvers in order of preference: CLARABEL > OSQP > SCS
        # ECOS not available, using installed solvers
        solvers_to_try = [cp.CLARABEL, cp.OSQP, cp.SCS]
        solver_used = None
        
        for solver in solvers_to_try:
            try:
                problem.solve(solver=solver, verbose=False)
                solver_used = solver
                break
            except Exception as e:
                logger.debug(f"Solver {solver} failed: {e}")
                continue
        
        if solver_used is None:
            logger.error("All solvers failed")
            # Fallback to equal weight
            weights_dict = {t: 1/n_assets for t in tickers}
            metadata = {
                'status': 'failed',
                'solver_status': 'All solvers failed',
                'objective_value': None,
                'expected_return': None,
                'volatility': None,
                'turnover': None
            }
            return weights_dict, metadata
        
        # Extract optimal weights
        if problem.status not in ['optimal', 'optimal_inaccurate']:
            logger.warning(f"Optimization status: {problem.status} (solver: {solver_used})")
            # Fallback to equal weight
            weights_dict = {t: 1/n_assets for t in tickers}
            metadata = {
                'status': problem.status,
                'solver_status': f"{solver_used}: {problem.status}",
                'objective_value': None,
                'expected_return': None,
                'volatility': None,
                'turnover': None
            }
            return weights_dict, metadata
        
        # Successful optimization
        optimal_weights = w.value
        weights_dict = {t: max(0, w_val) for t, w_val in zip(tickers, optimal_weights)}
        
        # Renormalize to ensure sum=1 (numerical precision)
        total = sum(weights_dict.values())
        weights_dict = {t: w/total for t, w in weights_dict.items()}
        
        # Calculate realized metrics
        realized_return = mu @ optimal_weights
        realized_variance = optimal_weights @ Sigma @ optimal_weights
        realized_volatility = np.sqrt(realized_variance) * np.sqrt(252)  # Annualize
        realized_turnover = np.sum(np.abs(optimal_weights - w_prev))
        
        metadata = {
            'status': 'optimal',
            'solver_status': f"{solver_used}: {problem.status}",
            'objective_value': problem.value,
            'expected_return': realized_return,
            'volatility': realized_volatility,
            'turnover': realized_turnover,
            'tracking_error': self._calculate_tracking_error(optimal_weights, tickers, Sigma) if self.constraints.tracking_error.enabled else None
        }
        
        logger.info(f"Optimization successful (solver: {solver_used}): Return={realized_return:.4f}, Vol={realized_volatility:.4f}, Turnover={realized_turnover:.4f}")
        
        return weights_dict, metadata
    
    def _calculate_tracking_error(
        self, 
        weights: np.ndarray, 
        tickers: list, 
        covariance_matrix: np.ndarray
    ) -> float:
        """
        Calculate tracking error vs benchmark.
        
        Args:
            weights: Portfolio weights
            tickers: List of tickers
            covariance_matrix: Covariance matrix
            
        Returns:
            float: Annualized tracking error
        """
        w_bench = np.array([
            self.constraints.tracking_error.benchmark_weights.get(t, 0) 
            for t in tickers
        ])
        w_active = weights - w_bench
        te_variance = w_active @ covariance_matrix @ w_active
        return np.sqrt(te_variance) * np.sqrt(252)  # Annualize


class BacktestOptimizer:
    """
    Wrapper for running optimizer over historical backtest.
    Handles rolling window optimization with lookback periods.
    """
    
    def __init__(
        self, 
        optimizer: TAAOptimizer,
        lookback_days: int = 252,  # 1 year of data for cov estimation
        rebalance_freq: str = 'W'  # Weekly rebalancing
    ):
        """
        Initialize backtest optimizer.
        
        Args:
            optimizer: TAAOptimizer instance
            lookback_days: Days of history for covariance estimation
            rebalance_freq: Rebalance frequency ('D', 'W', 'M')
        """
        self.optimizer = optimizer
        self.lookback_days = lookback_days
        self.rebalance_freq = rebalance_freq
    
    def run_backtest(
        self,
        predictions_df: pd.DataFrame,
        returns_df: pd.DataFrame,
        vix_series: Optional[pd.Series] = None
    ) -> pd.DataFrame:
        """
        Run backtest with periodic rebalancing.
        
        Args:
            predictions_df: DataFrame with predicted returns (Date index, ticker columns)
            returns_df: Historical returns for covariance estimation (Date index, ticker columns)
            vix_series: VIX time series for transaction cost adjustment
            
        Returns:
            pd.DataFrame: Portfolio weights over time with metadata columns
        """
        logger.info(f"Starting backtest: {len(predictions_df)} prediction dates, {len(returns_df)} return dates")
        logger.info(f"Predictions range: {predictions_df.index.min()} to {predictions_df.index.max()}")
        logger.info(f"Returns range: {returns_df.index.min()} to {returns_df.index.max()}")
        
        # Resample to rebalance frequency
        # Resample gets last available prediction in each period
        rebalanced_predictions = predictions_df.resample(self.rebalance_freq).last()
        # Drop any NaN rows (periods with no data)
        rebalanced_predictions = rebalanced_predictions.dropna(how='all')
        logger.info(f"Rebalance dates ({self.rebalance_freq}): {len(rebalanced_predictions)} dates")
        
        results = []
        previous_weights = None
        skipped_count = 0
        
        for i, date in enumerate(rebalanced_predictions.index):
            try:
                if i == 0:
                    logger.info(f"Processing first rebalance date: {date}")
                
                # Get predictions from resampled data
                expected_returns = rebalanced_predictions.loc[date].to_dict()
                
                # Calculate covariance matrix using lookback window
                lookback_start = date - pd.Timedelta(days=self.lookback_days)
                historical_returns = returns_df.loc[lookback_start:date]
                
                if len(historical_returns) < 20:  # Minimum data required
                    logger.warning(f"Insufficient data at {date} ({len(historical_returns)} days), skipping")
                    continue
                
                cov_matrix = historical_returns.cov() * 252  # Annualize
                
                # Get VIX if available
                current_vix = vix_series.loc[date] if vix_series is not None and date in vix_series.index else None
                
                # Optimize
                weights, metadata = self.optimizer.optimize(
                    expected_returns=expected_returns,
                    covariance_matrix=cov_matrix,
                    previous_weights=previous_weights,
                    current_vix=current_vix
                )
                
                # Store results
                result = {'Date': date}
                result.update(weights)
                result.update({f'meta_{k}': v for k, v in metadata.items()})
                results.append(result)
                
                previous_weights = weights
                
                if (i + 1) % 50 == 0:
                    logger.info(f"Processed {i + 1}/{len(rebalance_dates)} rebalance dates, {len(results)} successful")
            
            except Exception as e:
                logger.error(f"Error processing date {date}: {e}", exc_info=True)
                continue
        
        logger.info(f"Backtest complete: {len(results)} results, {skipped_count} skipped")
        
        if not results:
            raise RuntimeError(
                f"No valid optimization results generated. "
                f"Check that predictions and returns date ranges overlap. "
# =============================================================================
# COMPARISON OPTIMIZERS (Simplified for Testing)
# =============================================================================
# These optimizers are simplified versions for comparing different optimization
# approaches. They don't include full constraint support - use TAAOptimizer for
# production backtests with constraints.
# =============================================================================

class BaseOptimizer:
    """
    Base class for comparison optimizers.
    
    Note: These are simplified optimizers for testing different approaches.
    For production backtests with full constraints, use TAAOptimizer.
    """ex('Date')
        logger.info(f"Backtest complete: {len(results_df)} portfolio weights generated")
        
        return results_df


# =============================================================================
# Base Class for All Optimizers
# =============================================================================

class BaseOptimizer:
    """Base class for all portfolio optimizers."""
    
    def __init__(self, max_position: float = 0.40, min_position: float = 0.0):
        """
        Initialize base optimizer.
        
        Args:
            max_position: Maximum weight per asset (default 40%)
            min_position: Minimum weight per asset (default 0% for long-only)
        """
        self.max_position = max_position
        self.min_position = min_position
    
    def optimize(
        self,
        expected_returns: Dict[str, float],
        covariance_matrix: pd.DataFrame,
        previous_weights: Dict[str, float] = None
    ) -> Dict[str, float]:
        """
        Optimize portfolio weights. Override in subclasses.
        
        Args:
            expected_returns: Dict of ticker -> expected return
            covariance_matrix: Covariance matrix of returns
            previous_weights: Previous portfolio weights (for turnover)
            
        Returns:
            Dict[str, float]: Optimal weights (ticker -> weight)
        """
        raise NotImplementedError
    
    def _normalize_weights(self, weights: Dict[str, float]) -> Dict[str, float]:
        """Ensure weights sum to 1."""
        total = sum(weights.values())
        return {k: v/total for k, v in weights.items()} if total > 0 else weights


# =============================================================================
# Mean-Variance Optimizer (Current/Default)
# =============================================================================

class MeanVarianceOptimizer(BaseOptimizer):
    """Mean-Variance optimization with risk aversion parameter."""
    
    def __init__(self, risk_aversion: float = 1.0, **kwargs):
        """
        Initialize Mean-Variance optimizer.
        
        Args:
            risk_aversion: Risk aversion parameter (default 1.0)
                - Higher values → more conservative (lower variance)
                - Lower values → more aggressive (higher return focus)
            **kwargs: Additional base class arguments
        """
        super().__init__(**kwargs)
        self.risk_aversion = risk_aversion
    
    def optimize(self, expected_returns, covariance_matrix, previous_weights=None):
        tickers = list(expected_returns.keys())
        n = len(tickers)
        
        mu = np.array([expected_returns[t] for t in tickers])
        Sigma = covariance_matrix.loc[tickers, tickers].values
        
        w = cp.Variable(n)
        objective = cp.Maximize(mu @ w - self.risk_aversion * cp.quad_form(w, Sigma))
        constraints = [
            cp.sum(w) == 1,
            w >= self.min_position,
            w <= self.max_position
        ]
        
        problem = cp.Problem(objective, constraints)
        problem.solve(solver=cp.CLARABEL, verbose=False)
        
        if problem.status in ['optimal', 'optimal_inaccurate']:
            weights = {t: max(0, w_val) for t, w_val in zip(tickers, w.value)}
            return self._normalize_weights(weights)
        else:
            return {t: 1/n for t in tickers}


# =============================================================================
# Maximum Sharpe Ratio Optimizer
# =============================================================================

class MaxSharpeOptimizer(BaseOptimizer):
    """Maximize Sharpe ratio directly (risk-adjusted returns)."""
    
    def __init__(self, risk_free_rate: float = 0.02, **kwargs):
        """
        Initialize Maximum Sharpe optimizer.
        
        Args:
            risk_free_rate: Annual risk-free rate (default 2%)
            **kwargs: Additional base class arguments
        """
        super().__init__(**kwargs)
        self.rf = risk_free_rate / 252  # Daily rate
    
    def optimize(self, expected_returns, covariance_matrix, previous_weights=None):
        tickers = list(expected_returns.keys())
        n = len(tickers)
        
        mu = np.array([expected_returns[t] for t in tickers])
        Sigma = covariance_matrix.loc[tickers, tickers].values
        
        # Auxiliary variable trick for maximizing Sharpe ratio
        y = cp.Variable(n)
        kappa = cp.Variable()
        
        objective = cp.Maximize((mu - self.rf) @ y)
        constraints = [
            cp.quad_form(y, Sigma) <= 1,
            cp.sum(y) == kappa,
            kappa >= 0,
            y >= self.min_position * kappa,
            y <= self.max_position * kappa
        ]
        
        problem = cp.Problem(objective, constraints)
        problem.solve(solver=cp.CLARABEL, verbose=False)
        
        if problem.status in ['optimal', 'optimal_inaccurate'] and kappa.value > 1e-6:
            weights = {t: max(0, y_val / kappa.value) for t, y_val in zip(tickers, y.value)}
            return self._normalize_weights(weights)
        else:
            return {t: 1/n for t in tickers}


# =============================================================================
# Minimum Variance Optimizer
# =============================================================================

class MinVarianceOptimizer(BaseOptimizer):
    """Minimize portfolio variance (ignores expected returns)."""
    
    def optimize(self, expected_returns, covariance_matrix, previous_weights=None):
        tickers = list(expected_returns.keys())
        n = len(tickers)
        
        Sigma = covariance_matrix.loc[tickers, tickers].values
        
        w = cp.Variable(n)
        objective = cp.Minimize(cp.quad_form(w, Sigma))
        constraints = [
            cp.sum(w) == 1,
            w >= self.min_position,
            w <= self.max_position
        ]
        
        problem = cp.Problem(objective, constraints)
        problem.solve(solver=cp.CLARABEL, verbose=False)
        
        if problem.status in ['optimal', 'optimal_inaccurate']:
            weights = {t: max(0, w_val) for t, w_val in zip(tickers, w.value)}
            return self._normalize_weights(weights)
        else:
            return {t: 1/n for t in tickers}


# =============================================================================
# Risk Parity Optimizer
# =============================================================================

class RiskParityOptimizer(BaseOptimizer):
    """Equal risk contribution from each asset."""
    
    def optimize(self, expected_returns, covariance_matrix, previous_weights=None):
        tickers = list(expected_returns.keys())
        n = len(tickers)
        
        Sigma = covariance_matrix.loc[tickers, tickers].values
        
        # Risk contribution of asset i: w_i * (Sigma @ w)_i
        # Minimize sum of squared deviations from equal risk contribution
        def objective(w):
            portfolio_var = w @ Sigma @ w
            marginal_contrib = Sigma @ w
            risk_contrib = w * marginal_contrib
            target = portfolio_var / n
            return np.sum((risk_contrib - target) ** 2)
        
        constraints = [
            {'type': 'eq', 'fun': lambda w: np.sum(w) - 1},
        ]
        bounds = [(self.min_position, self.max_position) for _ in range(n)]
        
        result = minimize(
            objective,
            x0=np.ones(n) / n,
            method='SLSQP',
            bounds=bounds,
            constraints=constraints
        )
        
        if result.success:
            weights = {t: max(0, w_val) for t, w_val in zip(tickers, result.x)}
            return self._normalize_weights(weights)
        else:
            return {t: 1/n for t in tickers}


# =============================================================================
# Black-Litterman Optimizer
# =============================================================================

class BlackLittermanOptimizer(BaseOptimizer):
    """Blend ML predictions with market equilibrium (Black-Litterman model)."""
    
    def __init__(self, tau: float = 0.05, confidence: float = 0.5, **kwargs):
        """
        Initialize Black-Litterman optimizer.
        
        Args:
            tau: Uncertainty in prior (default 0.05)
            confidence: Confidence in ML predictions 0-1 (default 0.5)
            **kwargs: Additional base class arguments
        """
        super().__init__(**kwargs)
        self.tau = tau
        self.confidence = confidence
    
    def optimize(self, expected_returns, covariance_matrix, previous_weights=None):
        tickers = list(expected_returns.keys())
        n = len(tickers)
        
        Sigma = covariance_matrix.loc[tickers, tickers].values
        
        # Market equilibrium (equal weight as proxy)
        w_mkt = np.ones(n) / n
        
        # Implied returns from market weights (reverse optimization)
        pi = Sigma @ w_mkt
        
        # ML predictions as "views"
        Q = np.array([expected_returns[t] for t in tickers])
        P = np.eye(n)  # Absolute views (one per asset)
        
        # Uncertainty in views (inversely proportional to confidence)
        Omega = np.diag(np.diag(P @ (self.tau * Sigma) @ P.T)) / self.confidence
        
        # Black-Litterman formula
        M = P @ (self.tau * Sigma) @ P.T + Omega
        bl_returns = pi + (self.tau * Sigma) @ P.T @ np.linalg.inv(M) @ (Q - P @ pi)
        
        # Optimize with BL returns using mean-variance
        w = cp.Variable(n)
        objective = cp.Maximize(bl_returns @ w - cp.quad_form(w, Sigma))
        constraints = [
            cp.sum(w) == 1,
            w >= self.min_position,
            w <= self.max_position
        ]
        
        problem = cp.Problem(objective, constraints)
        problem.solve(solver=cp.CLARABEL, verbose=False)
        
        if problem.status in ['optimal', 'optimal_inaccurate']:
            weights = {t: max(0, w_val) for t, w_val in zip(tickers, w.value)}
            return self._normalize_weights(weights)
        else:
            return {t: 1/n for t in tickers}


# =============================================================================
# CVaR Optimizer
# =============================================================================

class CVaROptimizer(BaseOptimizer):
    """Minimize Conditional Value at Risk (tail risk protection)."""
    
    def __init__(self, alpha: float = 0.05, n_scenarios: int = 1000, **kwargs):
        """
        Initialize CVaR optimizer.
        
        Args:
            alpha: Confidence level (default 0.05 for 5% tail)
            n_scenarios: Number of Monte Carlo scenarios (default 1000)
            **kwargs: Additional base class arguments
        """
        super().__init__(**kwargs)
        self.alpha = alpha
        self.n_scenarios = n_scenarios
    
    def optimize(self, expected_returns, covariance_matrix, previous_weights=None):
        tickers = list(expected_returns.keys())
        n = len(tickers)
        
        mu = np.array([expected_returns[t] for t in tickers])
        Sigma = covariance_matrix.loc[tickers, tickers].values
        
        # Generate scenarios from multivariate normal
        np.random.seed(42)
        scenarios = np.random.multivariate_normal(mu, Sigma, self.n_scenarios)
        
        # CVaR optimization
        w = cp.Variable(n)
        alpha_var = cp.Variable()  # VaR threshold
        u = cp.Variable(self.n_scenarios)  # Auxiliary variables
        
        portfolio_returns = scenarios @ w
        
        # CVaR = VaR + (1/alpha) * E[max(0, VaR - R)]
        cvar = alpha_var + (1 / (self.alpha * self.n_scenarios)) * cp.sum(u)
        
        objective = cp.Minimize(-mu @ w + cvar)  # Maximize return, minimize CVaR
        constraints = [
            cp.sum(w) == 1,
            w >= self.min_position,
            w <= self.max_position,
            u >= 0,
            u >= alpha_var - portfolio_returns
        ]
        
        problem = cp.Problem(objective, constraints)
        problem.solve(solver=cp.CLARABEL, verbose=False)
        
        if problem.status in ['optimal', 'optimal_inaccurate']:
            weights = {t: max(0, w_val) for t, w_val in zip(tickers, w.value)}
            return self._normalize_weights(weights)
        else:
            return {t: 1/n for t in tickers}


# =============================================================================
# Hierarchical Risk Parity Optimizer
# =============================================================================

class HRPOptimizer(BaseOptimizer):
    """Hierarchical Risk Parity using machine learning clustering."""
    
    def optimize(self, expected_returns, covariance_matrix, previous_weights=None):
        tickers = list(expected_returns.keys())
        n = len(tickers)
        
        # Get correlation matrix
        corr_matrix = covariance_matrix.loc[tickers, tickers].corr()
        
        # Convert to distance matrix
        dist_matrix = np.sqrt((1 - corr_matrix) / 2)
        
        # Hierarchical clustering
        link = linkage(dist_matrix, method='single')
        
        # Get sorted clusters (quasi-diagonalization)
        def get_quasi_diag(link):
            link = link.astype(int)
            sort_ix = pd.Series([link[-1, 0], link[-1, 1]])
            num_items = link[-1, 3]
            while sort_ix.max() >= num_items:
                sort_ix.index = range(0, sort_ix.shape[0] * 2, 2)
                df0 = sort_ix[sort_ix >= num_items]
                i = df0.index
                j = df0.values - num_items
                sort_ix[i] = link[j, 0]
                df0 = pd.Series(link[j, 1], index=i + 1)
                sort_ix = pd.concat([sort_ix, df0]).sort_index()
                sort_ix.index = range(sort_ix.shape[0])
            return sort_ix.tolist()
        
        sort_ix = get_quasi_diag(link)
        sorted_tickers = [tickers[i] for i in sort_ix]
        
        # Recursive bisection for weights
        def get_cluster_var(cov, items):
            cov_slice = cov.loc[items, items]
            w = 1 / np.diag(cov_slice)
            w /= w.sum()
            return np.dot(w, np.dot(cov_slice, w))
        
        def get_recursive_bisection(cov, sort_ix):
            w = pd.Series(1.0, index=sort_ix)
            clusters = [sort_ix]
            
            while len(clusters) > 0:
                clusters = [c[j:k] for c in clusters 
                           for j, k in ((0, len(c) // 2), (len(c) // 2, len(c))) 
                           if len(c) > 1]
                
                for i in range(0, len(clusters), 2):
                    c0 = clusters[i]
                    c1 = clusters[i + 1]
                    
                    v0 = get_cluster_var(cov, c0)
                    v1 = get_cluster_var(cov, c1)
                    
                    alpha = 1 - v0 / (v0 + v1)
                    w[c0] *= alpha
                    w[c1] *= 1 - alpha
            
            return w
        
        w = get_recursive_bisection(covariance_matrix, sorted_tickers)
        
        # Apply position limits
        weights = {t: np.clip(w[t], self.min_position, self.max_position) for t in tickers}
        return self._normalize_weights(weights)


# =============================================================================
# Kelly Criterion Optimizer
# =============================================================================

class KellyOptimizer(BaseOptimizer):
    """Kelly Criterion - maximize expected log wealth (growth optimal)."""
    
    def __init__(self, kelly_fraction: float = 0.5, **kwargs):
        """
        Initialize Kelly optimizer.
        
        Args:
            kelly_fraction: Fraction of full Kelly (default 0.5 for safety)
                - 1.0 = Full Kelly (aggressive, max growth)
                - 0.5 = Half Kelly (safer, still good growth)
                - 0.25 = Quarter Kelly (conservative)
            **kwargs: Additional base class arguments
        """
        super().__init__(**kwargs)
        self.kelly_fraction = kelly_fraction
    
    def optimize(self, expected_returns, covariance_matrix, previous_weights=None):
        tickers = list(expected_returns.keys())
        n = len(tickers)
        
        mu = np.array([expected_returns[t] for t in tickers])
        Sigma = covariance_matrix.loc[tickers, tickers].values
        
        # Kelly optimal weights: w* = Sigma^-1 * mu
        # Use fractional Kelly for safety
        try:
            kelly_weights = np.linalg.solve(Sigma, mu)
            kelly_weights = kelly_weights * self.kelly_fraction
            
            # Clip to position limits
            kelly_weights = np.clip(kelly_weights, self.min_position, self.max_position)
            
            # Normalize
            if kelly_weights.sum() > 0:
                kelly_weights = kelly_weights / kelly_weights.sum()
            else:
                kelly_weights = np.ones(n) / n
            
            weights = {t: max(0, w_val) for t, w_val in zip(tickers, kelly_weights)}
            return self._normalize_weights(weights)
        except:
            return {t: 1/n for t in tickers}
